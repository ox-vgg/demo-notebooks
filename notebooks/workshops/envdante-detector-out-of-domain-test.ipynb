{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "blOJ1buw83T9",
   "metadata": {
    "id": "blOJ1buw83T9"
   },
   "source": [
    "# Envisioning Dante - Detection of Visual Elements with D526-v3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yLERLQAN3pM0",
   "metadata": {
    "id": "yLERLQAN3pM0"
   },
   "source": [
    "## 1 - Read Me First\n",
    "\n",
    "This project is a [Jupyter](https://jupyter.org/) notebook and was\n",
    "designed to run in [Google\n",
    "Colab](https://colab.research.google.com/).  If you are not reading\n",
    "this notebook in Google Colab, click\n",
    "[here](https://colab.research.google.com/github/ox-vgg/demo-notebooks/blob/main/notebooks/workshops/envdante-detector-out-of-domain-test.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-tfDPTizFHUi",
   "metadata": {
    "id": "-tfDPTizFHUi",
    "lines_to_next_cell": 2
   },
   "source": [
    "### 1.1 - What is, and how to use, a Jupyter notebook\n",
    "\n",
    "A Jupyter notebook is a series of \"cells\".  Each cell contains\n",
    "either text (like this one) or code (like others below).  A cell\n",
    "that contains code will have a \"Run cell\" button on the left side\n",
    "like this \"<img height=\"18rem\" alt=\"The 'Run cell' button in Colab\"\n",
    "src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAwCAQAAAD9CzEMAAABTklEQVRYw+2XMU7DMBRAX6ss3VA7VV25AFNWzsDQXoAzVDlBKw6QDJwhTO3OCVjaka0VXVKJDUVC4jOgiMHYcRx9S0j9f7XfS5x8+xsu8R9iQEpGyY4TgnBiR0lGyqA/fMaaI2LJI2tm4fAxObUV3mRNzjgEP+fcCm/yzLwbPKHwhjdZkPjiR2w64wVhw8jv6bdBeEHY+rxFEYz/WaiWWPTCC8LChZ9Q9RZUTOyCvDdeEHJ71drL6o43b0Ftq+6VYxJc8ciXp2L1F37IwSkAuOXVS3BgaApS55TfInzg00ORmoLMSwBww0urIDMFpbcAEpZ8OMeXpmDfQQBwzbNj/N6cUHUUANzzbi03I+oAAUx5stRCfIH6Eql/ZPXfVL3Q1LcK9c1OfbuOcOCoH5kRDn31tiVC4xWhdVRvfiO07xEuIFGuUBEugVGusZfQj28NImRviDLNnQAAAABJRU5ErkJggg==\">\".\n",
    "When you click the \"Run cell\" button the code in that cell will run.\n",
    "When it finishes running, a small green check mark appears next to\n",
    "the \"Run cell\" button\".  You need to wait for the code in that cell\n",
    "to finish before \"running\" the next cell.\n",
    "\n",
    "Typically, you run the cells one after the other since each cell is\n",
    "dependent on the results of the previous cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4kkvU97kDjYh",
   "metadata": {
    "id": "4kkvU97kDjYh"
   },
   "source": [
    "### 1.2 - Particulars of this notebook\n",
    "\n",
    "This notebook detects a series of page elements on images of book\n",
    "pages.  It was trained on 526 pages from 18 different volumes of\n",
    "Dante Alighieri's \"Comedy\", printed between 1470 and 1629.  It\n",
    "detects the following classes:\n",
    "\n",
    "  - catchword-signature\n",
    "  - graphic\n",
    "  - initial-capital\n",
    "  - manicules\n",
    "  - page-number\n",
    "  - poem\n",
    "  - prose\n",
    "  - running-header\n",
    "  - section-header\n",
    "  - sideletter\n",
    "  - sidenote\n",
    "  - unpainted-guideletter\n",
    "\n",
    "It takes as input a\n",
    "[VIA3](https://www.robots.ox.ac.uk/~vgg/software/via/app/via_image_annotator.html)\n",
    "project ID with a attribute named \"class\" with those class names as\n",
    "options.  It will report AP values for those annotations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yLHvmZjF5HWk",
   "metadata": {
    "id": "yLHvmZjF5HWk"
   },
   "source": [
    "### 1.3 - GPU access\n",
    "\n",
    "A GPU is not required to run this program but without a GPU it will\n",
    "run much slower.  Depending on the amount of data to analyse, it\n",
    "might not be sensible to use it without a GPU.  For reference, a\n",
    "single image takes about 0.2 seconds to analyse with a GPU.  On a\n",
    "CPU, the same image takes about 15 seconds (75 times slower).\n",
    "\n",
    "By default, this notebook will run with a GPU.  However, it is\n",
    "possible that you were not allocated one, typically because you've\n",
    "used up all your GPU resources.  You can confirm this, and possibly\n",
    "change it, manually.  To do that, navigate to \"Edit\" -> \"Notebook\n",
    "Settings\" and select \"GPU\" from the \"Hardware Accelerator\" menu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y7ZIaFAD1Org",
   "metadata": {
    "id": "y7ZIaFAD1Org"
   },
   "source": [
    "## 2 - Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tKauGGpqIi83",
   "metadata": {
    "id": "tKauGGpqIi83"
   },
   "source": [
    "### 2.1 - Check for GPU access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5h91qABuhv__",
   "metadata": {
    "cellView": "form",
    "id": "5h91qABuhv__"
   },
   "outputs": [],
   "source": [
    "#@markdown By default, this notebook will run with a GPU.  However, it\n",
    "#@markdown is possible that you were not allocated one.  If you get a\n",
    "#@markdown message saying that you do not have access to a GPU,\n",
    "#@markdown navigate to \"Edit\" -> \"Notebook Settings\" and select \"GPU\"\n",
    "#@markdown from the \"Hardware Accelerator\" menu.  If you change it,\n",
    "#@markdown you need to run this cell again.\n",
    "\n",
    "# We do this before everything else, namely before installing\n",
    "# detectron2 (which takes a lot of time), to identify early the case\n",
    "# of accidentally running this without a GPU.\n",
    "import torch.cuda\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    USE_GPU = True\n",
    "    print(\n",
    "        \"You are using GPU id %d: %s (%d GB)\"\n",
    "        % (\n",
    "            torch.cuda.current_device(),\n",
    "            torch.cuda.get_device_name(),\n",
    "            torch.cuda.get_device_properties(\n",
    "                torch.cuda.current_device()\n",
    "            ).total_memory\n",
    "            * 1e-9,\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    USE_GPU = False\n",
    "    print(\"You are NOT connected to a GPU\")\n",
    "    print(\"Consider reconnecting to a runtime with GPU access.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "U744F4c3s2Yp",
   "metadata": {
    "id": "U744F4c3s2Yp"
   },
   "source": [
    "### 2.2 - Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HYJHIkdn3Z1J",
   "metadata": {
    "cellView": "form",
    "id": "HYJHIkdn3Z1J"
   },
   "outputs": [],
   "source": [
    "#@markdown This step can take a few of minutes to finish.  It\n",
    "#@markdown downloads and installs software that is not available by\n",
    "#@markdown default on Google Colab.\n",
    "\n",
    "# Using `pip install --quiet` is not enough, it still prints out a\n",
    "# bunch of messages which is why we redirect stdout to `/dev/null`.\n",
    "# Important messages should go to stderr anyway.\n",
    "\n",
    "!pip install --quiet python-via > /dev/null\n",
    "\n",
    "# Detectron2 is not available on PyPI, we have to install it from\n",
    "# their git repos.\n",
    "!pip install --quiet git+https://github.com/facebookresearch/detectron2.git > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hF_7RBxmDCTx",
   "metadata": {
    "id": "hF_7RBxmDCTx"
   },
   "source": [
    "### 2.3 - Load dependencies and configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iimL6Db6gmbb",
   "metadata": {
    "cellView": "form",
    "id": "iimL6Db6gmbb",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#@markdown This cell prepares the detector to run.  This is the place\n",
    "#@markdown to make changes to the code if you want (but you should not\n",
    "#@markdown need to).\n",
    "\n",
    "DETECTRON2_CONFIG = \"https://thor.robots.ox.ac.uk/staging/env-dante/mask-rcnn-R-50-FPN-D526v3-2024-11-13.py\"\n",
    "MODEL_CKPT = \"https://thor.robots.ox.ac.uk/staging/env-dante/mask-rcnn-R-50-FPN-D526v3-2024-09-05.pth\"\n",
    "VIA3_VPS_URL = \"https://zeus.robots.ox.ac.uk/via/store/3.x.y/\"\n",
    "BATCH_SIZE = 1  # increasing this does not seem to make things any faster :/\n",
    "\n",
    "import contextlib\n",
    "import copy\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import os.path\n",
    "import re\n",
    "import time\n",
    "import urllib.parse\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, TypedDict\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import PIL.Image\n",
    "import detectron2.checkpoint\n",
    "import detectron2.config\n",
    "import detectron2.data\n",
    "import detectron2.data.catalog\n",
    "import detectron2.data.detection_utils\n",
    "import detectron2.data.transforms\n",
    "import detectron2.structures.masks\n",
    "import detectron2.utils.visualizer\n",
    "import numpy as np\n",
    "import pycocotools.coco\n",
    "import pycocotools.cocoeval\n",
    "import pycocotools.mask\n",
    "import requests\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import via.coco\n",
    "import via.vfs\n",
    "\n",
    "import google.colab.output\n",
    "import google.colab.files\n",
    "from google.colab.patches import cv2_imshow\n",
    "\n",
    "\n",
    "_logger = logging.getLogger()\n",
    "logging.basicConfig()\n",
    "\n",
    "\n",
    "## XXX: Detectron2 has not fixed, and does not look interested in\n",
    "## fixing, this issue\n",
    "## https://github.com/facebookresearch/detectron2/issues/3786\n",
    "## https://github.com/facebookresearch/detectron2/pull/4531 So silence\n",
    "## to avoid scaring users.\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"torch\\.meshgrid: in an upcoming release, it will be required to pass the indexing argument\\. \\(Triggered internally at \\.\\./aten/src/ATen/native/TensorShape\\.cpp\",\n",
    "    category=UserWarning,\n",
    "    module=\"torch.functional\",\n",
    ")\n",
    "\n",
    "\n",
    "## VIA3 still uses a VPS server and not VFS so we monkeypatch via.vfs\n",
    "## to support it\n",
    "class VPSProjectInfo(TypedDict):\n",
    "    pid: str\n",
    "    rev: str\n",
    "    rev_timestamp: str\n",
    "\n",
    "\n",
    "via.vfs.ProjectInfo = VPSProjectInfo\n",
    "via.vfs.is_project = lambda x: True\n",
    "\n",
    "\n",
    "def via3_make_files_local(via3_dict):\n",
    "    \"\"\"Beware! This edits via3_dict.\n",
    "\n",
    "    This downloads all files to cwd and edits file src and loc prefix.\n",
    "    \"\"\"\n",
    "    loc_prefix = via3_dict[\"config\"][\"file\"][\"loc_prefix\"]\n",
    "\n",
    "    fpaths = set()\n",
    "    url_to_fpath = {}\n",
    "\n",
    "    loc_prefix = via3_dict[\"config\"][\"file\"][\"loc_prefix\"]\n",
    "    for via3_file in via3_dict[\"file\"].values():\n",
    "        if via3_file[\"loc\"] == 4:  # inline\n",
    "            msg = f\"Inline images are not supported (fid {via3_file['fid']})\"\n",
    "            _logger.error(msg)\n",
    "            raise Exception(msg)\n",
    "        prefix = loc_prefix[str(via3_file[\"loc\"])]\n",
    "        url = prefix + via3_file[\"src\"]\n",
    "        if not re.match(\"https?://\", url):\n",
    "            msg = f\"All images must be over https (got '{url}')\"\n",
    "            _logger.error(msg)\n",
    "            raise Exception(msg)\n",
    "        fpath = os.path.basename(urllib.parse.urlparse(url).path)\n",
    "        if fpath in fpaths:\n",
    "            mgs = f\"Found two images with same filename {fpath}\"\n",
    "            _logger.error(msg)\n",
    "            raise Exception(msg)\n",
    "        url_to_fpath[url] = fpath\n",
    "        fpaths.add(fpath)\n",
    "\n",
    "    ## We download the images on a separate for loop so we can check\n",
    "    ## all filenames first and fail early if there are issues.\n",
    "    for via3_file in via3_dict[\"file\"].values():\n",
    "        prefix = loc_prefix[str(via3_file[\"loc\"])]\n",
    "        url = prefix + via3_file[\"src\"]\n",
    "        fpath = url_to_fpath[url]\n",
    "\n",
    "        r = requests.get(url)\n",
    "        if not r.ok:\n",
    "            msg = f\"failed to download image at URL '{url}': {r.reason}\"\n",
    "            _logger.error(msg)\n",
    "            raise Exception(msg)\n",
    "        with open(fpath, \"wb\") as fh:\n",
    "            _logger.info(\"downloading '%s' from '%s'\", fpath, url)\n",
    "            for chunk in r.iter_content(chunk_size=1024):\n",
    "                fh.write(chunk)\n",
    "        via3_file[\"src\"] = fpath\n",
    "\n",
    "    ## Empty the location prefix, everything is local now\n",
    "    for loc_prefix_key in loc_prefix.keys():\n",
    "        loc_prefix[loc_prefix_key] = \"\"\n",
    "\n",
    "    return fpaths  # won't be used, maybe helpful for debugging\n",
    "\n",
    "\n",
    "def display_stats(coco_dict, coco_results_dict):\n",
    "    coco_gt = pycocotools.coco.COCO()\n",
    "    coco_gt.dataset = copy.deepcopy(coco_dict)\n",
    "    with contextlib.redirect_stdout(open(os.devnull, \"w\")):\n",
    "        coco_gt.createIndex()\n",
    "\n",
    "    with contextlib.redirect_stdout(open(os.devnull, \"w\")):\n",
    "        coco_dt = coco_gt.loadRes(copy.deepcopy(coco_results_dict))\n",
    "\n",
    "    coco_eval = pycocotools.cocoeval.COCOeval(coco_gt, coco_dt, iouType=\"segm\")\n",
    "\n",
    "    ## Compute all\n",
    "    stats: Dict[str, Dict[str, float]] = {\"mAP\": {}, \"AP75\": {}, \"AP50\": {}}\n",
    "    with contextlib.redirect_stdout(open(os.devnull, \"w\")):\n",
    "        coco_eval.evaluate()\n",
    "        coco_eval.accumulate()\n",
    "        coco_eval.summarize()\n",
    "        stats[\"mAP\"][\"all\"] = coco_eval.stats[0]\n",
    "        stats[\"AP75\"][\"all\"] = coco_eval.stats[1]\n",
    "        stats[\"AP75\"][\"all\"] = coco_eval.stats[2]\n",
    "        for coco_cat in coco_gt.loadCats(coco_gt.getCatIds()):\n",
    "            coco_eval.params.catIds = [coco_cat[\"id\"]]\n",
    "            coco_eval.evaluate()\n",
    "            coco_eval.accumulate()\n",
    "            coco_eval.summarize()\n",
    "            stats[\"mAP\"][coco_cat[\"name\"]] = coco_eval.stats[0]\n",
    "            stats[\"AP50\"][coco_cat[\"name\"]] = coco_eval.stats[1]\n",
    "            stats[\"AP75\"][coco_cat[\"name\"]] = coco_eval.stats[2]\n",
    "\n",
    "    ## Print table header\n",
    "    cat_column_length = max(len(x) for x in stats[\"mAP\"].keys()) + 1\n",
    "    print(f'{\"Categories\":{cat_column_length}}    AP       AP75')\n",
    "    print(f'{\"-\"*cat_column_length}  -------  -------')\n",
    "    ## Print per category AP\n",
    "    for cat_name in sorted(stats[\"mAP\"].keys()):\n",
    "        if cat_name == \"all\":\n",
    "            continue\n",
    "        cat_mAP = stats[\"mAP\"][cat_name]\n",
    "        cat_AP75 = stats[\"AP75\"][cat_name]\n",
    "        cat_mAP = \"%.3f\" % cat_mAP if cat_mAP >= 0.0 else \"---\"\n",
    "        cat_AP75 = \"%.3f\" % cat_AP75 if cat_AP75 >= 0.0 else \"---\"\n",
    "        print(f\"{cat_name:{cat_column_length}}   {cat_mAP:^5}    {cat_AP75:^5}\")\n",
    "    ## Print mAP\n",
    "    print(f'{\"-\"*cat_column_length}  -------  -------')\n",
    "    all_mAP = stats[\"mAP\"][\"all\"]\n",
    "    all_AP75 = stats[\"AP75\"][\"all\"]\n",
    "    all_mAP = \"%.3f\" % all_mAP if all_mAP >= 0.0 else \"---\"\n",
    "    all_AP75 = \"%.3f\" % all_AP75 if all_AP75 >= 0.0 else \"---\"\n",
    "    print(f'{\"all\":{cat_column_length}}   {all_mAP:^5}   {all_AP75:^5}')\n",
    "\n",
    "\n",
    "class Detectron2DatasetFromFilelist(torch.utils.data.Dataset):\n",
    "    def __init__(self, fpath_list):\n",
    "        super().__init__()\n",
    "        self._fpath_list = fpath_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._fpath_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\"file_name\": self._fpath_list[idx]}\n",
    "\n",
    "\n",
    "def resize_pred_mask(mask, img_width, img_height):\n",
    "    \"\"\"Detectron2 may have resized the image and the mask comes on the\n",
    "    resized image size.  So resize the mask back to the image original\n",
    "    size.\n",
    "    \"\"\"\n",
    "    assert (\n",
    "        len(mask.shape) == 3\n",
    "        and mask.shape[0] == 1\n",
    "        and mask.dtype == torch.bool\n",
    "    )\n",
    "    original_img_size = (img_width, img_height)\n",
    "    mask_img = PIL.Image.fromarray(mask.cpu().numpy().reshape(mask.shape[1:]))\n",
    "    mask_img = mask_img.resize(original_img_size, resample=PIL.Image.BILINEAR)\n",
    "    mask_resized = np.array(mask_img, copy=False)\n",
    "    assert mask_resized.dtype == bool\n",
    "    return mask_resized\n",
    "\n",
    "\n",
    "def display_predictions(\n",
    "    coco_dict, coco_results_dict, score_thres, model_metadata\n",
    "):\n",
    "    coco_cat_id_to_color = {}\n",
    "    coco_cat_id_to_label = {}\n",
    "    for coco_cat in coco_dict[\"categories\"]:\n",
    "        thing_idx = model_metadata.thing_classes.index(coco_cat[\"name\"])\n",
    "        thing_colour = model_metadata.thing_colors[thing_idx]\n",
    "        coco_cat_id_to_color[coco_cat[\"id\"]] = thing_colour\n",
    "        coco_cat_id_to_label[coco_cat[\"id\"]] = coco_cat[\"name\"]\n",
    "\n",
    "    saved_images = []\n",
    "    for coco_img in coco_dict[\"images\"]:\n",
    "        coco_dets = [\n",
    "            x\n",
    "            for x in coco_results_dict\n",
    "            if x[\"image_id\"] == coco_img[\"id\"] and x[\"score\"] > score_thres\n",
    "        ]\n",
    "        pil_img = PIL.Image.open(coco_img[\"file_name\"])\n",
    "        np_img = np.asarray(pil_img)\n",
    "        vis = detectron2.utils.visualizer.Visualizer(\n",
    "            np_img,\n",
    "            instance_mode=detectron2.utils.visualizer.ColorMode.SEGMENTATION,\n",
    "        )\n",
    "        vis_out = vis.overlay_instances(\n",
    "            masks=[\n",
    "                pycocotools.mask.decode(x[\"segmentation\"]) for x in coco_dets\n",
    "            ],\n",
    "            labels=[coco_cat_id_to_label[x[\"category_id\"]] for x in coco_dets],\n",
    "            assigned_colors=[\n",
    "                coco_cat_id_to_color[x[\"category_id\"]] for x in coco_dets\n",
    "            ],\n",
    "        )\n",
    "        cv2_imshow(vis_out.get_image()[:, :, ::-1])\n",
    "        fpath_root, fpath_ext = os.path.splitext(coco_img[\"file_name\"])\n",
    "        save_fpath = fpath_root + f\"-preds-thres-{score_thres}\" + fpath_ext\n",
    "        vis_out.save(save_fpath)\n",
    "        saved_images.append(save_fpath)\n",
    "    return saved_images\n",
    "\n",
    "\n",
    "class BytesEncoderToASCII(json.JSONEncoder):\n",
    "    \"\"\"To encode RLE mask - plain decode to ascii\"\"\"\n",
    "\n",
    "    def default(self, o):\n",
    "        if isinstance(o, bytes):\n",
    "            return o.decode(\"ascii\")\n",
    "        else:\n",
    "            return super().default(o)\n",
    "\n",
    "\n",
    "def build_model(cfg):\n",
    "    model = detectron2.config.instantiate(cfg.model)\n",
    "    model = model.to(cfg.train.device)\n",
    "    checkpointer = detectron2.checkpoint.DetectionCheckpointer(model)\n",
    "    checkpointer.load(cfg.train.init_checkpoint)\n",
    "    return model\n",
    "\n",
    "\n",
    "cfg = detectron2.config.LazyConfig.load(\n",
    "    detectron2.utils.file_io.PathManager.get_local_path(DETECTRON2_CONFIG)\n",
    ")\n",
    "cfg.train.init_checkpoint = MODEL_CKPT\n",
    "if USE_GPU:\n",
    "    cfg.train.device = \"cuda\"\n",
    "else:\n",
    "    cfg.train.device = \"cpu\"\n",
    "\n",
    "model_metadata = detectron2.data.catalog.MetadataCatalog.get(\n",
    "    cfg.dataloader.test.dataset.names[0]\n",
    ")\n",
    "\n",
    "model = build_model(cfg)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AXRwfL1NAw6O",
   "metadata": {
    "id": "AXRwfL1NAw6O"
   },
   "source": [
    "## 3 - Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "H4Rw3Bv7RBYi",
   "metadata": {
    "id": "H4Rw3Bv7RBYi"
   },
   "source": [
    "### 3.1 - Download project and images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0FDT_4TUPzzW",
   "metadata": {
    "cellView": "form",
    "id": "0FDT_4TUPzzW",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#@markdown This cell will download the VIA shared project and\n",
    "#@markdown associated images, and will validate the annotations.\n",
    "#@markdown Check for any warning message which hint at possible\n",
    "#@markdown annotation errors such as regions without associated\n",
    "#@markdown category.\n",
    "\n",
    "# fmt: off\n",
    "PROJECT_UUID = \"\"  #@param {type: \"string\", placeholder: \"VIA project ID\"}\n",
    "# fmt: on\n",
    "\n",
    "via3_dict = via.vfs.download(VIA3_VPS_URL, PROJECT_UUID)\n",
    "via3_original_dict = copy.deepcopy(via3_dict)\n",
    "saved_images = via3_make_files_local(via3_dict)\n",
    "\n",
    "if set(via3_dict[\"attribute\"][\"class\"][\"options\"].values()) != set(\n",
    "    model_metadata.thing_classes\n",
    "):\n",
    "    raise Exception(\n",
    "        \"class options in VIA project are not the expected ones: got (%s) but expected (%s)\"\n",
    "        % (\n",
    "            list(via3_dict[\"attribute\"][\"class\"][\"options\"].values()),\n",
    "            model_metadata.thing_classes,\n",
    "        )\n",
    "    )\n",
    "\n",
    "coco_dict = via.coco.coco_dict_from_via3_dict(\n",
    "    via3_dict, image_basedir=\"\", category_attribute=\"class\"\n",
    ")\n",
    "\n",
    "## Map from Detectron2 predicted class ID (contiguous ID) to the COCO\n",
    "## category ID so we can construct COCO results file.\n",
    "pred_class_to_coco_cat_id = {}\n",
    "for coco_cat in coco_dict[\"categories\"]:\n",
    "    pred_class_id = model_metadata.thing_classes.index(coco_cat[\"name\"])\n",
    "    pred_class_to_coco_cat_id[pred_class_id] = coco_cat[\"id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hA_8RBxjDCTx",
   "metadata": {
    "id": "hA_8RBxjDCTx"
   },
   "source": [
    "### 3.2 - Run Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jjmL6Db6gmcc",
   "metadata": {
    "cellView": "form",
    "id": "jjmL6Db6gmcc"
   },
   "outputs": [],
   "source": [
    "#@markdown This cell runs the detector on all images in the VIA\n",
    "#@markdown project but will display nothing.  When it finishes\n",
    "#@markdown running, run the following cells to obtain detection\n",
    "#@markdown statistics and visualise the detections.\n",
    "\n",
    "coco_fname_to_img = {x[\"file_name\"]: x for x in coco_dict[\"images\"]}\n",
    "\n",
    "dataset = Detectron2DatasetFromFilelist(list(coco_fname_to_img.keys()))\n",
    "dataset_mapper = detectron2.config.instantiate(cfg.dataloader.test.mapper)\n",
    "dataloader = detectron2.data.build_detection_test_loader(\n",
    "    dataset,\n",
    "    mapper=dataset_mapper,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "\n",
    "coco_results_dict = []\n",
    "for inputs in dataloader:\n",
    "    with torch.no_grad():\n",
    "        start_compute_time = time.perf_counter()\n",
    "        outputs = model(inputs)\n",
    "        compute_time = time.perf_counter() - start_compute_time\n",
    "        _logger.debug(\n",
    "            \"Inference time: %f seconds (batch size %d)\",\n",
    "            compute_time,\n",
    "            BATCH_SIZE,\n",
    "        )\n",
    "    for input, output in zip(inputs, outputs):\n",
    "        coco_img = coco_fname_to_img[input[\"file_name\"]]\n",
    "        instances = output[\"instances\"].to(\"cpu\")\n",
    "        for i in range(len(instances)):\n",
    "            instance = instances[i]\n",
    "            coco_cat_id = pred_class_to_coco_cat_id[int(instance.pred_classes)]\n",
    "            mask = resize_pred_mask(\n",
    "                instance.pred_masks, coco_img[\"width\"], coco_img[\"height\"]\n",
    "            )\n",
    "            mask_rle = pycocotools.mask.encode(np.asfortranarray(mask))\n",
    "            coco_results_dict.append(\n",
    "                {\n",
    "                    \"image_id\": coco_img[\"id\"],\n",
    "                    \"category_id\": coco_cat_id,\n",
    "                    \"segmentation\": mask_rle,\n",
    "                    \"score\": float(instance.scores),\n",
    "                }\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hA_8RBxjDCTf",
   "metadata": {
    "id": "hA_8RBxjDCTf"
   },
   "source": [
    "### 3.3 - Compute and display statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jjmL6Db6gmdd",
   "metadata": {
    "cellView": "form",
    "id": "jjmL6Db6gmdd",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#@markdown This cell displays a table with the AP (average precision)\n",
    "#@markdown for each class at\n",
    "#@markdown [IoU](https://en.wikipedia.org/wiki/Jaccard_index) between\n",
    "#@markdown 0.50 and 0.95, as well as IoU at 0.75 only.\n",
    "\n",
    "display_stats(coco_dict, coco_results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hA_8RBxjDCTg",
   "metadata": {
    "id": "hA_8RBxjDCTg"
   },
   "source": [
    "### 3.3 - Display predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kkmL6Db6gee",
   "metadata": {
    "cellView": "form",
    "id": "kkmL6Db6gee",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#@markdown This cell will display the predicted regions (masks)\n",
    "#@markdown superimposed on the original images.  Each prediction has\n",
    "#@markdown an associated \"confidence score\" --- a value between 0.0\n",
    "#@markdown and 1.0.  You can adjust the confidence score threshold to\n",
    "#@markdown only show detections with a confidence score higher than\n",
    "#@markdown the threshold.\n",
    "\n",
    "# fmt: off\n",
    "CONFIDENCE_THRESHOLD = 0.5  #@param {type: \"slider\", min: 0.0, max: 1.0, step: 0.01}\n",
    "# fmt: on\n",
    "\n",
    "google.colab.output.no_vertical_scroll()\n",
    "pred_fpaths = display_predictions(\n",
    "    coco_dict, coco_results_dict, CONFIDENCE_THRESHOLD, model_metadata\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hB_8RBxjDCZg",
   "metadata": {
    "id": "hB_8RBxjDCZg"
   },
   "source": [
    "### 3.4 - Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "llmL6Db6gff",
   "metadata": {
    "cellView": "form",
    "id": "llmL6Db6gff"
   },
   "outputs": [],
   "source": [
    "#@markdown This cell prepares and downloads a zip file with the\n",
    "#@markdown original VIA project, the annotations in COCO format, the\n",
    "#@markdown original images, and the images with annotated predictions.\n",
    "\n",
    "results_zip_fpath = f\"d526-v3-at-thres-{CONFIDENCE_THRESHOLD}.zip\"\n",
    "with ZipFile(results_zip_fpath, \"w\") as zip_fh:\n",
    "    for pred_fpath in pred_fpaths:  # images with prediction masks\n",
    "        zip_fh.write(pred_fpath)\n",
    "\n",
    "    for saved_image in saved_images:  # original images\n",
    "        zip_fh.write(saved_image)\n",
    "\n",
    "    coco_data_fpath = \"coco-data.json\"\n",
    "    with open(coco_data_fpath, \"w\") as fh:\n",
    "        json.dump(coco_dict, fh)\n",
    "    zip_fh.write(coco_data_fpath)\n",
    "\n",
    "    coco_results_fpath = \"coco-results.json\"\n",
    "    with open(coco_results_fpath, \"w\") as fh:\n",
    "        json.dump(coco_results_dict, fh, cls=BytesEncoderToASCII)\n",
    "    zip_fh.write(coco_results_fpath)\n",
    "\n",
    "    via3_project_fpath = \"via3-annotations.json\"\n",
    "    with open(via3_project_fpath, \"w\") as fh:\n",
    "        json.dump(via3_original_dict, fh)\n",
    "    zip_fh.write(via3_project_fpath)\n",
    "\n",
    "google.colab.files.download(results_zip_fpath)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
