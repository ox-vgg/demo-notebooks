{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Get started with using WISE 2 in Colab\n",
        "<img src=\"https://gitlab.com/vgg/wise/wise/-/raw/wise2/docs/assets/natural_language_search.png\" alt=\"Screenshot of WISE online demo\" width=\"60%\"/>\n",
        "\n",
        "**This notebook allows you to easily use the WISE Search Engine on your own collection of images/videos (stored in a folder on Google Drive).**\n",
        "\n",
        "You can open this notebook in Google Colab using the button below:\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/ox-vgg/demo-notebooks/blob/main/notebooks/wise2-colab.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "If you would like to use WISE 2 locally on your own computer, see the [installation instructions in our GitLab repo](https://gitlab.com/vgg/wise/wise/-/blob/wise2/docs/Install.md).\n",
        "\n",
        "<!-- Check out our online demo of WISE with 5 million images at https://meru.robots.ox.ac.uk/unsplash/ -->\n",
        "\n",
        "For more details about WISE, please visit our [code repo](https://gitlab.com/vgg/wise/wise/-/blob/wise2/README.md) and [software page](https://www.robots.ox.ac.uk/~vgg/software/wise/). If you have any questions, please contact horacelee@robots.ox.ac.uk"
      ],
      "metadata": {
        "id": "fNf-LQusupYQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 - Read Me First\n",
        "\n",
        "This is a [Jupyter](https://jupyter.org/) notebook for using the WISE Image Search Engine on a collection of images, and is designed to run in [Google\n",
        "Colab](https://colab.research.google.com/).\n",
        "<!-- If you are not reading\n",
        "this notebook in Google Colab, click\n",
        "[here](https://colab.research.google.com/github/ox-vgg/follow-things-around/blob/main/tracking.ipynb). -->\n"
      ],
      "metadata": {
        "id": "GdyiRXTIwFR_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 - What is, and how to use, a Jupyter notebook\n",
        "\n",
        "A Jupyter notebook is a series of \"cells\".  Each cell contains either text (like this one) or code (like others below).  A cell that contains code will have a \"Run cell\" button on the left side like this\n",
        "\"<img height=\"18rem\" alt=\"The 'Run cell' button in Colab\"\n",
        "src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAwCAQAAAD9CzEMAAABTklEQVRYw+2XMU7DMBRAX6ss3VA7VV25AFNWzsDQXoAzVDlBKw6QDJwhTO3OCVjaka0VXVKJDUVC4jOgiMHYcRx9S0j9f7XfS5x8+xsu8R9iQEpGyY4TgnBiR0lGyqA/fMaaI2LJI2tm4fAxObUV3mRNzjgEP+fcCm/yzLwbPKHwhjdZkPjiR2w64wVhw8jv6bdBeEHY+rxFEYz/WaiWWPTCC8LChZ9Q9RZUTOyCvDdeEHJ71drL6o43b0Ftq+6VYxJc8ciXp2L1F37IwSkAuOXVS3BgaApS55TfInzg00ORmoLMSwBww0urIDMFpbcAEpZ8OMeXpmDfQQBwzbNj/N6cUHUUANzzbi03I+oAAUx5stRCfIH6Eql/ZPXfVL3Q1LcK9c1OfbuOcOCoH5kRDn31tiVC4xWhdVRvfiO07xEuIFGuUBEugVGusZfQj28NImRviDLNnQAAAABJRU5ErkJggg==\">\".\n",
        "When you click the \"Run cell\" button, the code in that cell will run and when it finishes, a green check mark appears next to the \"Run cell\" button\".  You need to wait for the code in that cell to finish before \"running\" the next cell.\n"
      ],
      "metadata": {
        "id": "R1LXkZ5uwIy6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 - Particulars of this notebook\n",
        "\n",
        "You must run the cells in this notebook one after the other since each cell is dependent on the results of the previous cell.\n",
        "\n",
        "This notebook also has some interactive cells, namely in the options sections.  After setting their values, these cells must be run, just like other code cells.  Setting their values only has effect after you \"run\" the cell."
      ],
      "metadata": {
        "id": "jvnFMmWNwOEd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 - Setup and Installation"
      ],
      "metadata": {
        "id": "z1r1TwcCwQ1z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 - Check for GPU access\n",
        "\n",
        "A GPU is highly recommended for faster processing speeds, as your videos might take a long time to process without a GPU."
      ],
      "metadata": {
        "id": "dmpOcOmERLG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown By default, this notebook will run with a GPU.  However, it\n",
        "#@markdown is possible that you were not allocated one, typically\n",
        "#@markdown because you've used up all your GPU resources.  If you get\n",
        "#@markdown a message saying that you do not have access to a GPU,\n",
        "#@markdown navigate to \"Edit\" -> \"Notebook Settings\" and select \"GPU\"\n",
        "#@markdown from the \"Hardware Accelerator\" menu.  Once you change it,\n",
        "#@markdown you need to run this cell again.\n",
        "\n",
        "import torch.cuda\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    DEVICE = \"cuda\"\n",
        "    print(\"You are using this GPU:\")\n",
        "    print(\n",
        "        \"GPU %d: %s (%d GB)\"\n",
        "        % (\n",
        "            torch.cuda.current_device(),\n",
        "            torch.cuda.get_device_name(),\n",
        "            torch.cuda.get_device_properties(\n",
        "                torch.cuda.current_device()\n",
        "            ).total_memory\n",
        "            * 1e-9,\n",
        "        )\n",
        "    )\n",
        "else:\n",
        "    DEVICE = \"cpu\"\n",
        "    print(\"You are not connected to a GPU.  This might be a bit slow.\")\n",
        "    print(\"Consider reconnecting to a runtime with GPU access.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "wowAOceKRFkC",
        "outputId": "4b3447cd-675b-44fd-b0f1-3986c10f9d79"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are using this GPU:\n",
            "GPU 0: Tesla T4 (15 GB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 - Install WISE"
      ],
      "metadata": {
        "id": "Czhib_K8xOo9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZbWbEbHDIOO",
        "outputId": "bb764ffd-3221-4d5c-8c62-6d5ac632c63e",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.4/109.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.7/253.7 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m91.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m705.5/705.5 kB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m107.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.2/233.2 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.8/91.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.0/409.0 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.5/313.5 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m120.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.8/74.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m93.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m105.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.7/425.7 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.1/164.1 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 3.2.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.39.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[K\u001b[?25h\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G\u001b[2K\u001b[1G"
          ]
        }
      ],
      "source": [
        "#@markdown This cell installs WISE and the software packages WISE uses. This will take 1-2 minutes.\n",
        "\n",
        "%cd -q /content\n",
        "# For now, clone the `wise2-image-video-frontend` branch as it hasn't been merged into `wise2` yet\n",
        "!git clone --quiet -b wise2-image-video-frontend https://gitlab.com/vgg/wise/wise.git\n",
        "!pip install --quiet -r wise/requirements.txt\n",
        "# !pip install --quiet open-clip-torch==2.26.1 # install a newer version of open-clip-torch to support MobileCLIP\n",
        "!pip install --quiet --no-deps msclap==1.3.3\n",
        "!pip install --quiet faiss-gpu==1.7.2\n",
        "\n",
        "# Temporary hack\n",
        "%cd -q /content/wise/frontend\n",
        "!npm install --silent\n",
        "!npm run build --silent -- --logLevel error\n",
        "\n",
        "%cd -q /content/wise"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 - Create a project with your images/videos"
      ],
      "metadata": {
        "id": "GnqFXrtrx1tc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 - Mount Google Drive or download an example set of images"
      ],
      "metadata": {
        "id": "4K7oVuyXSj29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title  {\"run\":\"auto\"}\n",
        "#@markdown You can choose to either:\n",
        "#@markdown 1. Mount your Google Drive (if you want to use WISE on your\n",
        "#@markdown own images/videos)\n",
        "#@markdown 2. Or you can download an example set of images (of cats and dogs)\n",
        "\n",
        "mount_drive_or_download_example = \"Download example images\" # @param [\"Mount Google Drive\",\"Download example images\"]\n",
        "\n",
        "#@markdown If you choose the Google Drive option, you will be asked to\n",
        "#@markdown grant access to your Google Drive Files.  This is required\n",
        "#@markdown to access the images/videos you want to use in WISE. Once\n",
        "#@markdown you click on \"Connect to Google Drive\", a pop-up window will\n",
        "#@markdown appear to choose a Google Account and then to allow access to\n",
        "#@markdown \"Google Drive for desktop\".\n",
        "\n",
        "if mount_drive_or_download_example == \"Mount Google Drive\":\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    print('Google Drive mounted. Please see your files in the `drive` folder in the \"Files\" menu in the left sidebar')\n",
        "else:\n",
        "    print(f'Downloading example images (Oxford-IIIT Pet Dataset)')\n",
        "    %cd -q /content\n",
        "    # Download .tar.gz file and extract images\n",
        "    # Delete images.tar.gz in case the user is re-running this cell\n",
        "    !rm -f images.tar.gz && \\\n",
        "      wget https://thor.robots.ox.ac.uk/~vgg/data/pets/images.tar.gz && \\\n",
        "      echo \"Extracting images\" && \\\n",
        "      tar -zxvf images.tar.gz | tqdm > /dev/null\n",
        "    %cd -q /content/wise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "pW0z-hF9SkiL",
        "outputId": "be12880f-e80c-4c84-d114-1bdf2a6ae956"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading example images (Oxford-IIIT Pet Dataset)\n",
            "--2024-11-04 12:38:50--  https://thor.robots.ox.ac.uk/~vgg/data/pets/images.tar.gz\n",
            "Resolving thor.robots.ox.ac.uk (thor.robots.ox.ac.uk)... 129.67.95.98\n",
            "Connecting to thor.robots.ox.ac.uk (thor.robots.ox.ac.uk)|129.67.95.98|:443... connected.\n",
            "HTTP request sent, awaiting response... 308 Permanent Redirect\n",
            "Location: https://thor.robots.ox.ac.uk/pets/images.tar.gz [following]\n",
            "--2024-11-04 12:38:51--  https://thor.robots.ox.ac.uk/pets/images.tar.gz\n",
            "Reusing existing connection to thor.robots.ox.ac.uk:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 791918971 (755M) [application/octet-stream]\n",
            "Saving to: ‘images.tar.gz’\n",
            "\n",
            "images.tar.gz       100%[===================>] 755.23M  25.4MB/s    in 31s     \n",
            "\n",
            "2024-11-04 12:39:22 (24.5 MB/s) - ‘images.tar.gz’ saved [791918971/791918971]\n",
            "\n",
            "Extracting images\n",
            "7394it [00:09, 784.17it/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 - Choose a folder of images/videos"
      ],
      "metadata": {
        "id": "1MmgNbkQ1oL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Specify the folder of images/videos you want to use:\n",
        "#@markdown\n",
        "#@markdown - **If you mounted your Google Drive**, open the \"Files\" tab\n",
        "#@markdown (<img height=\"18rem\" alt=\"Files icon\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAGAAAABgCAYAAADimHc4AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAYKADAAQAAAABAAAAYAAAAACpM19OAAADqElEQVR4Ae2csYsTQRSHd3ZDjiPIWVraaCHCEThsPbC8OjZyehjUNMHG3lxvoSAYA9Hzzi611wlnJ3Jw+h/Y2IogAQ23GWdMSGDJzsw+n3Fm/V21O7Pvl7ffl02yOTJRhD8QAAEQAAEQAAEQAAEQAAEQAAEQAAEQAAEQAAEQAAEQAIGyExBFT/Bms3VFjsdNKaLNRIjzUSSqRTOKHC/TcWd/r7dbpCakY50FNBqNarV29kklSe6pE3Su44AhZLS796Lb4cjyLSNxaajdbq+kIjlM4sp1dfxS4f/uT11t9fqG+HhyfOTSb0jHOAm4cGn96RT+vzu3kkqwCtCv+UkcdxX55T/zs7pLKCHOnmN2X7/hegF/2ph683+4c7vVyfYZ6r5dgHrW+XZyZZJgFTD5qOmbgigqiwTr6/qtZkua8L/qd60Zpvrs3Pb2g1qyMnwjpbyanfNzX45SKT+rj8pHIo77+/3uhyJ9Wq+AImEcxx4cPBqmP2tbQoh3HHl/P0NUExFfjOP4rnomvr+xc+eZvmdyfVzvBOjGw5Mwwy3UjWpr9czaob53mo0aNrwUoPsNWEIkROXa1+8/Hhu4z6a8FRC6BP2Vjb6HmpHO2fBaQOASxPQeKgf9ZNh7ASFL0N8YG+mrySAEhCrB5R6qYjPk07x+Y1b9bPrUk/k+yf6/kmCuAJ+gc/YCAZw0CVkQQIDGWQIBnDQJWRBAgMZZAgGcNAlZEECAxlkCAZw0CVkQQIDGWQIBnDQJWRBAgMZZAgGcNAlZEECAxlkCAZw0CVkQQIDGWQIBnDQJWRBAgMZZAgGcNAlZEECAxlkCAZw0CVkQQIDGWQIBnDQJWRBAgMZZAgGcNAlZEECAxlkCAZw0CVkQQIDGWQIBnDQJWRBAgMZZAgGcNAlZEECAxlkCAZw0CVkQQIDGWQIBnDQJWRBAgMZZAgGcNAlZEECAxlkCAZw0CVkQQIDGWeLwO2E5Mq0Nav6dLGerIWZpduY/6xWgFyMyR2A2j4ALO6sAvRJU3gNg3EzAhZ1dgFqGSz2Mcdkycxv/7azUS5jZzt66buink+Mvl9fr59SSXBu2MMzPCZym6fPXL3u9+cjiLesVoMtGw2/3pTx9uzgCo1kCmpVmlh1ftO8kYDAYjNZqq1vKql5BFy9Hi0hOxqRmpFlpZvmHzWcKLzm57OXr5636uvVny1b6elboCwRAAARAAARAAARAAARAAARAAARAAARAAARAAARAAARAgI/AL2RB1YUYmnX7AAAAAElFTkSuQmCC\" />)\n",
        "#@markdown in the left sidebar. The `drive` folder contains your\n",
        "#@markdown Google Drive files.  Navigate the files, right click on the\n",
        "#@markdown desired folder containing your images/videos, and select\n",
        "#@markdown \"Copy path\".  Then paste the path in the MEDIA_DIRECTORY field below.\n",
        "#@markdown Do not forget to then \"run\" this cell.\n",
        "#@markdown\n",
        "#@markdown - **If you downloaded the example images**, enter\n",
        "#@markdown \"/[]()content/images/\" in the MEDIA_DIRECTORY field below:\n",
        "\n",
        "#@markdown Be careful not to accidentally delete any files in your Google Drive!\n",
        "\n",
        "MEDIA_DIRECTORY = \"/content/images\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Don't forget to run this cell after entering the filepath!\n",
        "\n",
        "import os\n",
        "if not MEDIA_DIRECTORY:\n",
        "    raise Exception('MEDIA_DIRECTORY is empty, you must set it.')\n",
        "if not os.path.isdir(MEDIA_DIRECTORY):\n",
        "    raise Exception(\n",
        "        'The MEDIA_DIRECTORY \\'%s\\' does not exist or is not a valid folder' % MEDIA_DIRECTORY\n",
        "    )"
      ],
      "metadata": {
        "cellView": "form",
        "id": "_X9vD50a1ZOd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ## 3.3 Initialise a project from the folder of images/videos\n",
        "\n",
        "#@markdown This step creates a WISE project and processes your images using a\n",
        "#@markdown vision model (and an audio model if you videos contain audio).\n",
        "\n",
        "#@markdown Enter a folder path where you would like the project to be saved\n",
        "#@markdown in, or leave it as it is:\n",
        "PROJECT_DIRECTORY = \"/content/my-project\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### Vision model selection\n",
        "#@markdown We support a wide range of models from [OpenCLIP](https://github.com/mlfoundations/open_clip).\n",
        "#@markdown The default model is \"ViT-B-32:datacomp_xl_s13b_b90k\", but feel free to select another model\n",
        "#@markdown (e.g. you can use a larger model such as \"xlm-roberta-large-ViT-H-14:frozen_laion5b_s13b_b90k\"\n",
        "#@markdown for more accurate results).\n",
        "\n",
        "VISION_MODEL = \"ViT-B-32:datacomp_xl_s13b_b90k\" # @param ['RN50:openai', 'RN50:yfcc15m', 'RN50:cc12m', 'RN50-quickgelu:openai', 'RN50-quickgelu:yfcc15m', 'RN50-quickgelu:cc12m', 'RN101:openai', 'RN101:yfcc15m', 'RN101-quickgelu:openai', 'RN101-quickgelu:yfcc15m', 'RN50x4:openai', 'RN50x16:openai', 'RN50x64:openai', 'ViT-B-32:openai', 'ViT-B-32:laion400m_e31', 'ViT-B-32:laion400m_e32', 'ViT-B-32:laion2b_e16', 'ViT-B-32:laion2b_s34b_b79k', 'ViT-B-32:datacomp_xl_s13b_b90k', 'ViT-B-32:datacomp_m_s128m_b4k', 'ViT-B-32:commonpool_m_clip_s128m_b4k', 'ViT-B-32:commonpool_m_laion_s128m_b4k', 'ViT-B-32:commonpool_m_image_s128m_b4k', 'ViT-B-32:commonpool_m_text_s128m_b4k', 'ViT-B-32:commonpool_m_basic_s128m_b4k', 'ViT-B-32:commonpool_m_s128m_b4k', 'ViT-B-32:datacomp_s_s13m_b4k', 'ViT-B-32:commonpool_s_clip_s13m_b4k', 'ViT-B-32:commonpool_s_laion_s13m_b4k', 'ViT-B-32:commonpool_s_image_s13m_b4k', 'ViT-B-32:commonpool_s_text_s13m_b4k', 'ViT-B-32:commonpool_s_basic_s13m_b4k', 'ViT-B-32:commonpool_s_s13m_b4k', 'ViT-B-32-256:datacomp_s34b_b86k', 'ViT-B-32-quickgelu:openai', 'ViT-B-32-quickgelu:laion400m_e31', 'ViT-B-32-quickgelu:laion400m_e32', 'ViT-B-32-quickgelu:metaclip_400m', 'ViT-B-32-quickgelu:metaclip_fullcc', 'ViT-B-16:openai', 'ViT-B-16:laion400m_e31', 'ViT-B-16:laion400m_e32', 'ViT-B-16:laion2b_s34b_b88k', 'ViT-B-16:datacomp_xl_s13b_b90k', 'ViT-B-16:datacomp_l_s1b_b8k', 'ViT-B-16:commonpool_l_clip_s1b_b8k', 'ViT-B-16:commonpool_l_laion_s1b_b8k', 'ViT-B-16:commonpool_l_image_s1b_b8k', 'ViT-B-16:commonpool_l_text_s1b_b8k', 'ViT-B-16:commonpool_l_basic_s1b_b8k', 'ViT-B-16:commonpool_l_s1b_b8k', 'ViT-B-16:dfn2b', 'ViT-B-16-quickgelu:metaclip_400m', 'ViT-B-16-quickgelu:metaclip_fullcc', 'ViT-B-16-plus-240:laion400m_e31', 'ViT-B-16-plus-240:laion400m_e32', 'ViT-L-14:openai', 'ViT-L-14:laion400m_e31', 'ViT-L-14:laion400m_e32', 'ViT-L-14:laion2b_s32b_b82k', 'ViT-L-14:datacomp_xl_s13b_b90k', 'ViT-L-14:commonpool_xl_clip_s13b_b90k', 'ViT-L-14:commonpool_xl_laion_s13b_b90k', 'ViT-L-14:commonpool_xl_s13b_b90k', 'ViT-L-14-quickgelu:metaclip_400m', 'ViT-L-14-quickgelu:metaclip_fullcc', 'ViT-L-14-quickgelu:dfn2b', 'ViT-L-14-336:openai', 'ViT-H-14:laion2b_s32b_b79k', 'ViT-H-14-quickgelu:metaclip_fullcc', 'ViT-H-14-quickgelu:dfn5b', 'ViT-H-14-378-quickgelu:dfn5b', 'ViT-g-14:laion2b_s12b_b42k', 'ViT-g-14:laion2b_s34b_b88k', 'ViT-bigG-14:laion2b_s39b_b160k', 'roberta-ViT-B-32:laion2b_s12b_b32k', 'xlm-roberta-base-ViT-B-32:laion5b_s13b_b90k', 'xlm-roberta-large-ViT-H-14:frozen_laion5b_s13b_b90k', 'convnext_base:laion400m_s13b_b51k', 'convnext_base_w:laion2b_s13b_b82k', 'convnext_base_w:laion2b_s13b_b82k_augreg', 'convnext_base_w:laion_aesthetic_s13b_b82k', 'convnext_base_w_320:laion_aesthetic_s13b_b82k', 'convnext_base_w_320:laion_aesthetic_s13b_b82k_augreg', 'convnext_large_d:laion2b_s26b_b102k_augreg', 'convnext_large_d_320:laion2b_s29b_b131k_ft', 'convnext_large_d_320:laion2b_s29b_b131k_ft_soup', 'convnext_xxlarge:laion2b_s34b_b82k_augreg', 'convnext_xxlarge:laion2b_s34b_b82k_augreg_rewind', 'convnext_xxlarge:laion2b_s34b_b82k_augreg_soup', 'coca_ViT-B-32:laion2b_s13b_b90k', 'coca_ViT-B-32:mscoco_finetuned_laion2b_s13b_b90k', 'coca_ViT-L-14:laion2b_s13b_b90k', 'coca_ViT-L-14:mscoco_finetuned_laion2b_s13b_b90k', 'EVA01-g-14:laion400m_s11b_b41k', 'EVA01-g-14-plus:merged2b_s11b_b114k', 'EVA02-B-16:merged2b_s8b_b131k', 'EVA02-L-14:merged2b_s4b_b131k', 'EVA02-L-14-336:merged2b_s6b_b61k', 'EVA02-E-14:laion2b_s4b_b115k', 'EVA02-E-14-plus:laion2b_s9b_b144k', 'ViT-B-16-SigLIP:webli', 'ViT-B-16-SigLIP-256:webli', 'ViT-B-16-SigLIP-i18n-256:webli', 'ViT-B-16-SigLIP-384:webli', 'ViT-B-16-SigLIP-512:webli', 'ViT-L-16-SigLIP-256:webli', 'ViT-L-16-SigLIP-384:webli', 'ViT-SO400M-14-SigLIP:webli', 'ViT-SO400M-14-SigLIP-384:webli', 'ViT-L-14-CLIPA:datacomp1b', 'ViT-L-14-CLIPA-336:datacomp1b', 'ViT-H-14-CLIPA:datacomp1b', 'ViT-H-14-CLIPA-336:laion2b', 'ViT-H-14-CLIPA-336:datacomp1b', 'ViT-bigG-14-CLIPA:datacomp1b', 'ViT-bigG-14-CLIPA-336:datacomp1b', 'nllb-clip-base:v1', 'nllb-clip-large:v1', 'nllb-clip-base-siglip:v1', 'nllb-clip-large-siglip:v1']\n",
        "\n",
        "VISION_MODEL = f\"mlfoundations/open_clip/{'/'.join(VISION_MODEL.split(':'))}\"\n",
        "\n",
        "#@markdown ### Audio model selection\n",
        "#@markdown Currently we only support 1 audio model (Microsoft CLAP)\n",
        "AUDIO_MODEL = \"Microsoft CLAP\" # @param ['Microsoft CLAP']\n",
        "\n",
        "#@markdown If this step is too slow, you can use a GPU to speed up the image processing.\n",
        "#@markdown To do that, go to \"Runtime\" -> \"Change runtime type\" and select a GPU option such as \"T4 GPU\" under \"Hardware accelerator\".\n",
        "#@markdown You will need to re-run the cells in this notebook from the beginning.\n",
        "\n",
        "ok_to_create_project = True\n",
        "if os.path.exists(PROJECT_DIRECTORY):\n",
        "    print(f\"Project folder already exists at {PROJECT_DIRECTORY}\")\n",
        "    user_input = input('Would you like to delete the project folder and re-create the project (yes/no):')\n",
        "\n",
        "    yes_choices = ['yes', 'y']\n",
        "    no_choices = ['no', 'n']\n",
        "\n",
        "    if user_input.lower() in yes_choices:\n",
        "        !rm -rf \"{PROJECT_DIRECTORY}\"\n",
        "    elif user_input.lower() in no_choices:\n",
        "        ok_to_create_project = False\n",
        "    else:\n",
        "        print('Invalid input. Please type yes or no')\n",
        "        ok_to_create_project = False\n",
        "\n",
        "if ok_to_create_project:\n",
        "    %cd -q /content/wise\n",
        "    !python3 extract-features.py \\\n",
        "      \"{MEDIA_DIRECTORY}\" \\\n",
        "      --project-dir \"{PROJECT_DIRECTORY}\" \\\n",
        "      --image-feature-id \"{VISION_MODEL}\" \\\n",
        "      --video-feature-id \"{VISION_MODEL}\"\n",
        "\n",
        "\"\"\"\n",
        "Inference speed (CPU) on Kinetics-6 videos (with audio feature extraction as well)\n",
        "MobileCLIP-B/datacompdr_lt: 591it in 447s = 1.32it/s\n",
        "\n",
        "Inference speed (CPU) on Pets dataset:\n",
        "- xlm-roberta-large-ViT-H-14/frozen_laion5b_s13b_b90k: around 0.12im/s\n",
        "- ViT-B-32/openai: around 3.1im/s\n",
        "- MobileCLIP-B/datacompdr_lt: around 1.37im/s\n",
        "- MobileCLIP-S2/datacompdr: around 1.58im/s\n",
        "- MobileCLIP-S1/datacompdr: around 2.17im/s\n",
        "\n",
        "Inference speed (T4 GPU) on Pets dataset:\n",
        "- ViT-B-32/openai: around 45im/s\n",
        "- MobileCLIP-S1/datacompdr: around 20im/s\n",
        "\n",
        "Note: model wasn't reparameterized (inference speed might be faster with reparameterization)\n",
        "\"\"\";"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGhg7wmKIHBU",
        "outputId": "2692540b-51dd-463c-e320-0d455485290d",
        "cellView": "form"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
            "Initialising internal metadata database\n",
            "2024-11-04 12:40:22,700 (MainThread): root - INFO - Skipping 3 files that are not valid media in directory \"/content/images\"\n",
            "100% 7390/7390 [00:03<00:00, 1977.50it/s]\n",
            "2024-11-04 12:40:26,445 (MainThread): root - INFO - Initialising feature extractor\n",
            "2024-11-04 12:40:26,451 (MainThread): root - INFO - Loaded ViT-B-32 model config.\n",
            "open_clip_pytorch_model.bin: 100% 605M/605M [00:03<00:00, 159MB/s]\n",
            "2024-11-04 12:40:32,499 (MainThread): root - INFO - Loading pretrained ViT-B-32 weights (datacomp_xl_s13b_b90k).\n",
            "Using mlfoundations/open_clip/ViT-B-32/datacomp_xl_s13b_b90k for image\n",
            "# writing /content/my-project/store/mlfoundations/open_clip/ViT-B-32/datacomp_xl_s13b_b90k/features/image-000000.tar 0 0.0 GB 0\n",
            "Initializing data loader with 0 workers ...\n",
            "Feature extraction: 7390it [02:32, 48.41it/s]\n",
            "Feature extraction completed in 200 sec (3.33 min)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ## 3.4 Create a search index\n",
        "#@markdown This step creates a search index to enable fast and accurate search.\n",
        "\n",
        "#@markdown If you have a large collection of images (e.g. more than 1 million images),\n",
        "#@markdown you can change the index type to 'IndexIVFFlat' to enable faster search times at the cost of slightly reduced retrieval accuracy.\n",
        "#@markdown Otherwise you can leave this setting at its default value ('IndexFlatIP').\n",
        "INDEX_TYPE = \"IndexFlatIP\" # @param [\"IndexFlatIP\", \"IndexIVFFlat\"]\n",
        "\n",
        "%cd -q /content/wise\n",
        "!python3 create-index.py \\\n",
        "  --project-dir \"{PROJECT_DIRECTORY}\" \\\n",
        "  --index-type {INDEX_TYPE}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "9-k_2MHlIIB5",
        "outputId": "0840806d-606b-4047-db63-fbfa917b042c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
            "Adding feature vectors to index\n",
            "100% 7390/7390 [00:01<00:00, 7352.16it/s]\n",
            "  saved index to /content/my-project/store/mlfoundations/open_clip/ViT-B-32/datacomp_xl_s13b_b90k/index/image-IndexFlatIP.faiss\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 - Search with WISE web interface"
      ],
      "metadata": {
        "id": "XDX18GmhJWT5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Display the web interface for WISE so that you can perform searches.\n",
        "#@markdown\n",
        "#@markdown **Once you see the web interface show up below, you can start searching\n",
        "#@markdown there** (scroll down a bit if needed), even if it looks like this cell\n",
        "#@markdown is still loading/running.\n",
        "\n",
        "%cd -q /content/wise\n",
        "from pathlib import Path\n",
        "from google.colab.output import serve_kernel_port_as_iframe\n",
        "\n",
        "%env HOSTNAME=0.0.0.0\n",
        "from api import serve\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "import logging\n",
        "logging.disable(logging.INFO)\n",
        "\n",
        "def callback():\n",
        "  print()\n",
        "  print('Server started. You can start searching with WISE below (scroll down a bit to see the full interface)')\n",
        "  serve_kernel_port_as_iframe(9670, path=f'/{Path(PROJECT_DIRECTORY).stem}/', height=800)\n",
        "\n",
        "print('Starting server')\n",
        "serve(Path(PROJECT_DIRECTORY), index_type=INDEX_TYPE, theme_asset_dir='frontend/dist', callback=callback)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "P0IJE6avJn7y",
        "outputId": "1fb2c2ea-e9a6-4e02-d8da-56f78e7e4b11",
        "cellView": "form"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: HOSTNAME=0.0.0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting server\n",
            "WARNING: WISE currently does not support serving multiple projects simultaneously.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00,  7.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Server started. You can start searching with WISE below (scroll down a bit to see the full interface)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "[{'url': '/content/my-project/store/mlfoundations/ base: None name: None nfiles: 4 nbytes: 30300160 samples: 7390 cache: /tmp/_wids_cache\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "    const iframe = document.createElement('iframe');\n",
              "    iframe.src = new URL(path, url).toString();\n",
              "    iframe.height = height;\n",
              "    iframe.width = width;\n",
              "    iframe.style.border = 0;\n",
              "    iframe.allow = [\n",
              "        'accelerometer',\n",
              "        'autoplay',\n",
              "        'camera',\n",
              "        'clipboard-read',\n",
              "        'clipboard-write',\n",
              "        'gyroscope',\n",
              "        'magnetometer',\n",
              "        'microphone',\n",
              "        'serial',\n",
              "        'usb',\n",
              "        'xr-spatial-tracking',\n",
              "    ].join('; ');\n",
              "    element.appendChild(iframe);\n",
              "  })(9670, \"/my-project/\", \"100%\", 800, false, window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shutting down\n",
            "shutting down\n",
            "shutting down\n"
          ]
        }
      ]
    }
  ]
}
